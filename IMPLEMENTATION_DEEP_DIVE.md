# CRITICAL IMPLEMENTATION GUIDE
## Phase 0 Deep Dive - Code Examples & Architecture

---

## PROBLEM #1: Rule-Based Intent Inference is Fragile

### Current Implementation (BROKEN)

```python
# control.py - Current (BAD)
def infer_controls(description: str) -> dict:
    d = description.lower()
    
    sender = "user"
    recipient = "recipient"
    intent = "general"
    confidence = "low"
    
    if any(k in d for k in ["student", "exam", "fee", "college"]):
        sender = "student"
        recipient = "academic office"
        intent = "fee_related"
        confidence = "high"  # ‚Üê PROBLEM: Based on 1 keyword!
```

### Failure Cases

```python
# Case 1: Negation ignored
desc = "I'm NOT a student, just a parent confused about the fee structure"
‚Üí Detected as: student ‚Üí fee_related (WRONG)

# Case 2: Context mismatch
desc = "As a payment consultant, I help companies collect fees"
‚Üí Detected as: client ‚Üí payment (WRONG - should be corporate)

# Case 3: Paraphrasing
desc = "I need to ask the academic department about my tuition costs"
‚Üí Detected as: NOT matched (missed because no "fee", "college", "student" exact match)
‚Üí Classified as: general (WRONG - should be fee_related)
```

### Solution: ML-Based Classifier

```python
# NEW: intent_classifier.py (GOOD)

from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

class IntentClassifier:
    """
    Robust, ML-based email intent classifier.
    Handles paraphrasing, negation, context better than keyword matching.
    """
    
    def __init__(self):
        self.pipeline = None
        self.intent_labels = {
            0: {"intent": "fee_related", "sender": "student", "recipient": "academic_office"},
            1: {"intent": "leave_request", "sender": "employee", "recipient": "manager"},
            2: {"intent": "payment", "sender": "client", "recipient": "accounts"},
            3: {"intent": "general", "sender": "user", "recipient": "recipient"},
        }
        self._train_or_load()
    
    def _train_or_load(self):
        """Train on seed data or load cached model."""
        # Training data: (description, label)
        TRAINING_DATA = [
            # Fee related (label 0)
            ("my college fee is pending, need an extension", 0),
            ("fee waiver request for financial hardship", 0),
            ("I have to pay the semester fees urgently", 0),
            ("our school fees are too high", 0),
            ("cost of the course is unclear to me", 0),
            
            # Leave related (label 1)
            ("I need sick leave for medical appointment", 1),
            ("request time off next week for family emergency", 1),
            ("my manager needs to approve my leave", 1),
            ("can't come to work, have doctor appointment", 1),
            
            # Payment related (label 2)
            ("invoice payment is due on the 15th", 2),
            ("kindly process this payment request", 2),
            ("payment reminder for project completion", 2),
            ("the client hasn't paid yet", 2),
            
            # General/ambiguous (label 3)
            ("write a professional email", 3),
            ("help me draft a message", 3),
            ("I need to write something", 3),
            ("what's a good way to start?", 3),
        ]
        
        texts = [item[0] for item in TRAINING_DATA]
        labels = [item[1] for item in TRAINING_DATA]
        
        # Build pipeline: TF-IDF ‚Üí Naive Bayes
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                max_features=100,
                ngram_range=(1, 2),  # Unigrams and bigrams (handles phrases)
                min_df=1,
                max_df=0.8
            )),
            ('clf', MultinomialNB(alpha=1.0))
        ])
        
        self.pipeline.fit(texts, labels)
    
    def classify(self, description: str) -> dict:
        """
        Classify email and return controls.
        
        Returns:
            {
                "intent": "fee_related" | "leave_request" | "payment" | "general",
                "sender": "student" | "employee" | "client" | "user",
                "recipient": "academic_office" | "manager" | "accounts" | "recipient",
                "confidence": 0.0 - 1.0,  # Model's confidence in prediction
                "domain": "education" | "hr" | "corporate" | "other",
            }
        """
        try:
            # Get prediction and confidence score
            label = self.pipeline.predict([description])[0]
            confidence_scores = self.pipeline.predict_proba([description])[0]
            confidence = float(confidence_scores[label])
            
            result = self.intent_labels[label].copy()
            result["confidence"] = confidence
            result["domain"] = self._domain_from_intent(result["intent"])
            
            return result
        
        except Exception as e:
            logger.error(f"Classifier error: {e}, using fallback")
            # Fallback to safe defaults
            return {
                "intent": "general",
                "sender": "user",
                "recipient": "recipient",
                "confidence": 0.0,
                "domain": "other",
            }
    
    def _domain_from_intent(self, intent: str) -> str:
        """Map intent to domain."""
        mapping = {
            "fee_related": "education",
            "leave_request": "hr",
            "payment": "corporate",
            "general": "other",
        }
        return mapping.get(intent, "other")

# Usage in control.py
classifier = IntentClassifier()

def infer_controls(description: str) -> dict:
    result = classifier.classify(description)
    
    # Use continuous confidence (0.0-1.0) instead of binary
    confidence_level = "high" if result['confidence'] > 0.6 else "low"
    
    return {
        "tone": "formal",
        "length": "short",
        "sender": result['sender'],
        "recipient": result['recipient'],
        "intent": result['intent'],
        "domain": result['domain'],
        "confidence": confidence_level,
        "confidence_score": result['confidence'],  # For logging
    }
```

### Why This Works Better

| Problem | Old (Keywords) | New (ML) |
|---------|---|---|
| **Paraphrasing** | Breaks ("tuition" vs "fee") | Handles via TF-IDF embeddings |
| **Negation** | Ignored ("NOT a student" still matches) | Learns from context ("not a student" ‚Üí low prob) |
| **Confidence Scoring** | Binary, unreliable | 0.0-1.0, calibrated |
| **New Patterns** | Requires code change | Improves with more training data |
| **Ambiguity** | Forces high/low choice | Returns uncertainty (0.4-0.6) |

---

## PROBLEM #2: No Output Content Validation

### Current Implementation (DANGEROUS)

```python
# service.py - Current (BAD)
def generate_email_service(description: str):
    validate_description(description)  # Only checks input length
    
    controls = infer_controls(description)
    prompt = build_prompt(controls, description)
    raw_output = call_llm(prompt)
    parsed = safe_parse_json(raw_output)
    
    return EmailResponse(**parsed)  # Returns LLM output unvalidated!
```

### Failure Cases

```python
# Case 1: Inappropriate tone
Input: "Tell my boss I deserve a raise"
Output: {
  "subject": "DEMANDING IMMEDIATE SALARY INCREASE",
  "body": "You MUST give me 50% more or I QUIT",
  "..."
}
# ‚Üê User sends this, gets fired

# Case 2: PII leakage
Input: "Email the finance team about my payment"
Output: {
  "body": "My SSN is 123-45-6789 and my account is ..."
}
# ‚Üê Accidental PII exposure

# Case 3: Hallucinated promises
Input: "Tell my client we're ready"
Output: {
  "body": "We guarantee delivery by December 15, 2025..."
}
# ‚Üê LLM invented a date that's wrong

# Case 4: Placeholder text left in
Input: "Email the manager"
Output: {
  "body": "Dear [MANAGER_NAME], [CONTEXT NEEDED]..."
}
# ‚Üê Incomplete email template
```

### Solution: Multi-Layer Output Validation

```python
# NEW: output_validator.py (GOOD)

import re
import logging

logger = logging.getLogger(__name__)

class OutputValidator:
    """
    Multi-layer validation to catch inappropriate, incomplete, or unsafe outputs.
    """
    
    # Profanity list (expand as needed)
    AGGRESSIVE_WORDS = {
        'demand', 'threat', 'resign', 'lawsuit', 'sue', 'hate',
        'stupid', 'idiot', 'incompetent', 'useless', 'fire me'
    }
    
    # Patterns indicating incomplete placeholders
    PLACEHOLDER_PATTERNS = [
        r'\[.*?\]',              # [NAME], [DATE], etc.
        r'\{.*?\}',              # {placeholder}
        r'XXX|YYY|ZZZ',         # Generic placeholders
        r'<.*?>',               # <name>, <email>, etc.
        r'___+',                # _____, underscores
    ]
    
    # PII patterns to flag (don't allow unless context justifies)
    PII_PATTERNS = {
        'ssn': r'\d{3}-\d{2}-\d{4}',
        'phone': r'(\d{3})[-.\s]?(\d{3})[-.\s]?(\d{4})',
        'credit_card': r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b',
    }
    
    @staticmethod
    def validate(email: EmailResponse, context: dict) -> tuple[bool, str]:
        """
        Comprehensive email output validation.
        
        Args:
            email: Generated EmailResponse object
            context: Controls dict with intent, tone, confidence, etc.
        
        Returns:
            (is_valid, reason)
        """
        
        # Check 1: Field length constraints
        checks = [
            OutputValidator._check_field_lengths(email),
            OutputValidator._check_placeholders(email),
            OutputValidator._check_aggressive_tone(email, context),
            OutputValidator._check_pii_leakage(email),
            OutputValidator._check_malformed_content(email),
            OutputValidator._check_consistency(email, context),
        ]
        
        for is_valid, reason in checks:
            if not is_valid:
                return False, reason
        
        return True, "Valid"
    
    @staticmethod
    def _check_field_lengths(email: EmailResponse) -> tuple[bool, str]:
        """Check field length constraints."""
        constraints = {
            'subject': (5, 100),
            'greeting': (5, 50),
            'body': (20, 1000),
            'closing': (5, 50),
        }
        
        for field, (min_len, max_len) in constraints.items():
            text = getattr(email, field, "")
            if not text or len(text) < min_len or len(text) > max_len:
                return False, f"{field} length invalid: {len(text)} chars (expected {min_len}-{max_len})"
        
        return True, ""
    
    @staticmethod
    def _check_placeholders(email: EmailResponse) -> tuple[bool, str]:
        """Detect incomplete placeholder text."""
        full_text = f"{email.subject} {email.body} {email.greeting} {email.closing}"
        
        for pattern in OutputValidator.PLACEHOLDER_PATTERNS:
            matches = re.findall(pattern, full_text, re.IGNORECASE)
            if matches:
                # Exception: [Name], [Date] are OK if email has low confidence
                # (user expects to fill these in)
                placeholder = matches[0]
                if placeholder.lower() in ['[name]', '[date]', '[amount]']:
                    # Low-confidence emails can have these
                    logger.info(f"Placeholder detected (acceptable): {placeholder}")
                else:
                    return False, f"Incomplete placeholder detected: {placeholder}"
        
        return True, ""
    
    @staticmethod
    def _check_aggressive_tone(email: EmailResponse, context: dict) -> tuple[bool, str]:
        """Detect aggressive or inappropriate tone."""
        full_text = (email.subject + ' ' + email.body).lower()
        
        # Rule 1: Excessive caps (> 30% uppercase)
        caps_count = sum(1 for c in full_text if c.isupper())
        caps_ratio = caps_count / max(len(full_text), 1)
        if caps_ratio > 0.3:
            return False, f"Email appears to be in ALL CAPS (aggressive, {caps_ratio:.0%})"
        
        # Rule 2: Aggressive words
        for word in OutputValidator.AGGRESSIVE_WORDS:
            if word in full_text:
                return False, f"Aggressive language detected: '{word}'"
        
        # Rule 3: Multiple exclamation marks (more than 2)
        if full_text.count('!') > 2:
            return False, f"Excessive exclamation marks ({full_text.count('!')})"
        
        # Rule 4: Formal context but informal tone
        if context.get('tone') == 'formal':
            informal = ['yo', 'lol', 'btw', 'imho', 'hey dude', 'omg']
            for marker in informal:
                if marker in full_text:
                    return False, f"Informal tone in formal context: '{marker}'"
        
        return True, ""
    
    @staticmethod
    def _check_pii_leakage(email: EmailResponse) -> tuple[bool, str]:
        """Detect potentially sensitive PII in email."""
        full_text = email.subject + ' ' + email.body + ' ' + email.closing
        
        # Check each PII pattern
        for pii_type, pattern in OutputValidator.PII_PATTERNS.items():
            matches = re.findall(pattern, full_text)
            if matches:
                logger.warning(f"PII detected ({pii_type}): {matches}")
                return False, f"Potential {pii_type} exposure in email"
        
        # Exception: Allow up to 1 email address (for context)
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b', 
                           full_text)
        if len(emails) > 1:
            return False, f"Too many email addresses ({len(emails)})"
        
        return True, ""
    
    @staticmethod
    def _check_malformed_content(email: EmailResponse) -> tuple[bool, str]:
        """Detect malformed email content."""
        
        # Check 1: Unmatched quotes/brackets
        for field in ['subject', 'body', 'greeting', 'closing']:
            text = getattr(email, field, "")
            if text.count('"') % 2 != 0:
                return False, f"{field} has unmatched quotes"
            if text.count('[') != text.count(']'):
                return False, f"{field} has unmatched brackets"
        
        # Check 2: Sensible structure (greeting + body + closing)
        if not email.greeting or not email.body or not email.closing:
            return False, "Email missing key sections (greeting/body/closing)"
        
        return True, ""
    
    @staticmethod
    def _check_consistency(email: EmailResponse, context: dict) -> tuple[bool, str]:
        """Check tone consistency with context."""
        
        intent = context.get('intent', 'general')
        confidence = context.get('confidence', 'low')
        
        # High-confidence emails should be well-formed, complete
        if confidence == 'high':
            if len(email.body) < 50:
                return False, "High-confidence email is too short"
        
        # Fee-related emails should be respectful
        if intent == 'fee_related':
            if any(word in email.body.lower() for word in ['complain', 'hate', 'unfair']):
                return False, "Fee email uses confrontational language"
        
        return True, ""

# Usage in service.py
def generate_email_service(description: str) -> dict:
    request_id = generate_request_id()
    
    try:
        validate_description(description)
        
        controls = infer_controls(description)
        prompt = build_prompt(controls, description)
        
        raw_output = call_llm(prompt)
        parsed = safe_parse_json(raw_output)
        
        email = EmailResponse(**parsed)
        
        # NEW: Validate output before returning
        is_valid, reason = OutputValidator.validate(email, controls)
        
        if not is_valid:
            logger.warning(f"[{request_id}] Output validation failed: {reason}")
            # Option 1: Return email anyway but log warning
            # Option 2: Regenerate with stricter prompt
            # Option 3: Return error to user with guidance
            
            # For now, return with warning
            return {
                "request_id": request_id,
                "email": email,
                "validation_warning": reason,
            }
        
        return {
            "request_id": request_id,
            "email": email,
            "validation_status": "passed",
        }
    
    except Exception as e:
        logger.error(f"[{request_id}] Error: {e}")
        raise
```

### Validation Results

```python
# Test Case 1: Aggressive tone
email = EmailResponse(
    subject="DEMANDING RAISE",
    greeting="Boss,",
    body="YOU MUST GIVE ME 50% MORE NOW!!!",
    closing="Or else"
)
is_valid, reason = OutputValidator.validate(email, {"tone": "formal"})
# is_valid = False
# reason = "Email appears to be in ALL CAPS (aggressive)"

# Test Case 2: With placeholders (low confidence OK)
email = EmailResponse(
    subject="Request for Extension",
    greeting="Dear [Professor Name],",
    body="I kindly request an extension to [DATE].",
    closing="Thank you"
)
is_valid, reason = OutputValidator.validate(email, {"confidence": "low"})
# is_valid = True (placeholders acceptable for low-confidence)

# Test Case 3: PII leakage
email = EmailResponse(
    subject="Payment",
    greeting="Dear Finance Team,",
    body="My SSN is 123-45-6789 and credit card ends in 4567.",
    closing="Thanks"
)
is_valid, reason = OutputValidator.validate(email, {"tone": "formal"})
# is_valid = False
# reason = "Potential ssn exposure in email"
```

---

## PROBLEM #3: Ollama Unavailability = Total Failure

### Current Implementation (FRAGILE)

```python
# llm/client.py - Current (BAD)
def call_llm(prompt: str) -> str:
    payload = {...}
    response = requests.post(OLLAMA_URL, json=payload, timeout=120)
    response.raise_for_status()  # ‚Üê Crashes if Ollama down
    data = response.json()
    return data["message"]["content"]
```

### Failure Cases

```python
# Scenario 1: Ollama service crashes
‚Üí requests.ConnectionError
‚Üí No retry, no fallback
‚Üí User sees 500 error immediately

# Scenario 2: Network timeout
‚Üí requests.Timeout after 120 seconds
‚Üí User waits 2 minutes just to see an error

# Scenario 3: Ollama overloaded
‚Üí Request takes 100+ seconds
‚Üí Multiple concurrent users all block
```

### Solution: Resilient Client with Retries & Fallbacks

```python
# NEW: llm/client.py (GOOD)

import requests
import time
import logging
from typing import Optional

logger = logging.getLogger(__name__)

OLLAMA_URL = "http://127.0.0.1:11434/api/chat"
MODEL_NAME = "mistral"
MAX_RETRIES = 3
BASE_TIMEOUT = 60  # seconds (reduced from 120)

class LLMClientError(Exception):
    """Base exception for LLM client errors."""
    pass

class LLMUnavailableError(LLMClientError):
    """LLM service is unavailable."""
    pass

def call_llm(prompt: str) -> str:
    """
    Call LLM with automatic retries and graceful degradation.
    
    Args:
        prompt: The prompt to send to LLM
    
    Returns:
        The LLM response
    
    Raises:
        LLMUnavailableError: If LLM unavailable after all retries
        LLMClientError: Other errors
    """
    return _call_llm_with_retries(prompt, attempt=0)

def _call_llm_with_retries(prompt: str, attempt: int) -> str:
    """Recursive retry helper with exponential backoff."""
    
    try:
        logger.info(f"LLM request (attempt {attempt+1}/{MAX_RETRIES+1})")
        
        start_time = time.time()
        
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": MODEL_NAME,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {
                    "num_predict": 120,
                    "temperature": 0.2
                }
            },
            timeout=BASE_TIMEOUT
        )
        
        elapsed_ms = (time.time() - start_time) * 1000
        logger.info(f"LLM responded in {elapsed_ms:.0f}ms")
        
        # Successful response
        response.raise_for_status()
        data = response.json()
        
        return data["message"]["content"]
    
    except (requests.Timeout, requests.ConnectionError) as e:
        # Transient errors: retry with backoff
        if attempt < MAX_RETRIES:
            backoff_secs = 2 ** attempt  # 1s, 2s, 4s
            logger.warning(
                f"LLM transient error (attempt {attempt+1}): {type(e).__name__}. "
                f"Retrying in {backoff_secs}s..."
            )
            time.sleep(backoff_secs)
            return _call_llm_with_retries(prompt, attempt + 1)
        else:
            logger.error(f"LLM unavailable after {MAX_RETRIES} retries")
            raise LLMUnavailableError(
                f"LLM service unavailable. "
                f"Tried {MAX_RETRIES+1} times over {sum(2**i for i in range(MAX_RETRIES))}s. "
                f"Last error: {e}"
            )
    
    except requests.HTTPError as e:
        # HTTP errors: don't retry (5xx might, 4xx won't)
        if 500 <= e.response.status_code < 600:
            if attempt < MAX_RETRIES:
                backoff_secs = 2 ** attempt
                logger.warning(f"LLM server error ({e.response.status_code}), "
                             f"retrying in {backoff_secs}s")
                time.sleep(backoff_secs)
                return _call_llm_with_retries(prompt, attempt + 1)
        
        logger.error(f"LLM HTTP error: {e}")
        raise LLMClientError(f"LLM HTTP error: {e}")
    
    except Exception as e:
        logger.error(f"Unexpected LLM error: {e}")
        raise LLMClientError(f"Unexpected error calling LLM: {e}")

# OPTIONAL: Fallback template generator
def generate_fallback_email(description: str, controls: dict) -> dict:
    """
    Generate a basic email using templates when LLM is unavailable.
    Better than failing completely.
    """
    logger.warning("Using fallback email generation (LLM unavailable)")
    
    templates = {
        "fee_related": {
            "subject": "Inquiry Regarding Fee Payment",
            "greeting": "Dear Academic Office,",
            "body": (
                "I am writing to inquire about my account. "
                "Please let me know the details and next steps. "
                "Thank you for your assistance."
            ),
            "closing": "Best regards"
        },
        "leave_request": {
            "subject": "Leave Request",
            "greeting": "Dear Manager,",
            "body": (
                "I would like to request leave as per company policy. "
                "Please advise on approval process and documentation needed."
            ),
            "closing": "Thank you"
        },
        "payment": {
            "subject": "Payment Status Inquiry",
            "greeting": "Dear Finance Team,",
            "body": (
                "I am writing to follow up on a pending payment. "
                "Please confirm receipt and timeline for processing."
            ),
            "closing": "Regards"
        },
        "general": {
            "subject": "Professional Inquiry",
            "greeting": "Dear Recipient,",
            "body": (
                "I am reaching out regarding a matter of importance. "
                "I would appreciate your attention and response."
            ),
            "closing": "Thank you"
        }
    }
    
    intent = controls.get('intent', 'general')
    template = templates.get(intent, templates['general'])
    
    return template

# Usage in service.py
def generate_email_service(description: str) -> dict:
    request_id = generate_request_id()
    
    try:
        validate_description(description)
        controls = infer_controls(description)
        prompt = build_prompt(controls, description)
        
        try:
            raw_output = call_llm(prompt)
        except LLMUnavailableError as e:
            logger.error(f"[{request_id}] LLM unavailable: {e}")
            
            # Fallback: Generate basic template
            parsed = generate_fallback_email(description, controls)
            parsed["_fallback"] = True
            parsed["_fallback_reason"] = str(e)
        else:
            parsed = safe_parse_json(raw_output)
            parsed["_fallback"] = False
        
        email = EmailResponse(**parsed)
        
        return {
            "request_id": request_id,
            "email": email,
            "fallback": parsed.get("_fallback", False),
        }
    
    except Exception as e:
        logger.error(f"[{request_id}] Error: {e}")
        raise
```

### Resilience Guarantees

```python
# Scenario 1: Single transient error
‚Üí Retries after 1 second
‚Üí Succeeds on retry ‚úÖ

# Scenario 2: Multiple transient errors
‚Üí Retries: 1s delay ‚Üí 2s delay ‚Üí 4s delay
‚Üí Succeeds on 3rd attempt ‚úÖ

# Scenario 3: Ollama completely down
‚Üí 3 retries over ~7 seconds
‚Üí Returns fallback email
‚Üí User sees response in < 10 seconds ‚úÖ (instead of 500 error)

# Scenario 4: Network restored mid-request
‚Üí Automatic retry succeeds
‚Üí No user-facing error ‚úÖ
```

---

## SUMMARY: Why These 3 Changes Matter

| Change | Impact | Effort | Priority |
|--------|--------|--------|----------|
| **ML Intent Classifier** | Fixes core accuracy issue | 8h | üî¥ CRITICAL |
| **Output Validation** | Prevents bad emails | 6h | üî¥ CRITICAL |
| **Resilient LLM Client** | Prevents 100% failure | 3h | üî¥ CRITICAL |

**Total Effort**: ~17 hours  
**Release Blocker**: YES - Cannot go to production without these

---

## Testing Checklist

```python
# test_intent_classifier.py
def test_classifier_handles_paraphrasing():
    classifier = IntentClassifier()
    
    # Different ways to say "I need a fee extension"
    texts = [
        "my college fee is pending",
        "I need to pay tuition",
        "the cost of the course is unclear",
        "fee waiver for financial hardship",
    ]
    
    # All should classify as fee_related
    for text in texts:
        result = classifier.classify(text)
        assert result['intent'] == 'fee_related'

# test_output_validator.py
def test_rejects_aggressive_email():
    email = EmailResponse(
        subject="DEMANDING IMMEDIATE RESPONSE",
        greeting="Sir,",
        body="YOU MUST DO THIS NOW!!!",
        closing="Or else"
    )
    is_valid, reason = OutputValidator.validate(email, {"tone": "formal"})
    assert not is_valid
    assert "CAPS" in reason or "aggressive" in reason

# test_llm_client.py
def test_retries_on_timeout(monkeypatch):
    call_count = [0]
    
    def mock_post(*args, **kwargs):
        call_count[0] += 1
        if call_count[0] < 3:
            raise requests.Timeout()
        # 3rd call succeeds
        return MockResponse('{"message": {"content": "OK"}}')
    
    monkeypatch.setattr(requests, 'post', mock_post)
    
    result = call_llm("test prompt")
    assert result == "OK"
    assert call_count[0] == 3  # Verified retried
```

---

**Next: Begin with ML classifier (P0.3). It's the highest-impact item.**
